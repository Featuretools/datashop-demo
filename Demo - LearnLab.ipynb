{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions from a LearnLab dataset\n",
    "<p style=\"margin:30px\">\n",
    "    <img width=50% src=\"https://www.featuretools.com/wp-content/uploads/2017/12/FeatureLabs-Logo-Tangerine-800.png\" alt=\"Featuretools\" />\n",
    "</p>\n",
    "\n",
    "In this tutorial, we show how to use [Featuretools](www.featuretools.com) on the standard LearnLab dataset structure. The workflow shown here can be used to quickly **organize** and **make predictions** about any LearnLab dataset.\n",
    "\n",
    "*If you're running this notebook yourself, please download the [geometry dataset](https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=76) into the `data` folder in this repository. You will only need the `.txt` file. The infrastructure in this notebook will work with **any** learnlab dataset, but you will need to change the filename in the following cell.*\n",
    "\n",
    "## Highlights\n",
    "* Show how to import a LearnLab dataset into featuretools\n",
    "* Show how to make custom primitives for stacking\n",
    "* Show efficacy of automatic feature generation with these datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Featuretools version 0.1.17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import featuretools as ft\n",
    "import utilities\n",
    "print('Using Featuretools version {}'.format(ft.__version__))\n",
    "data = pd.read_csv('data/ds2174_tx_All_Data_3991_2017_1128_123859.txt', '\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Creating a useful dataset structure\n",
    "At the beginning of any project, it is worthwhile to take a moment to think about how your dataset is structured.\n",
    "\n",
    "In these datasets the unique events come from `transactions`: places where a student interacts with a system. However, the columns of those transactions have variables that can be grouped together. \n",
    "\n",
    "For instance, there are only 78 distinct `problem_steps` for the 6778 transactions we in the geometry dataset. Associated to each problem step, we have a variety of knowledge components (KC) and custom fields (CF).\n",
    "\n",
    "We create an entityset structure using the `learnlab_to_entityset` function in [utilities](utilities.py). If you're interested in how `learnlab_to_entityset` is structured, there's an associated notebook [entityset_function](entityset_function.ipynb) explains choices made in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: Dataset\n",
       "  Entities:\n",
       "    transactions (shape = [6778, 27])\n",
       "    problem_steps (shape = [78, 49])\n",
       "    problems (shape = [20, 1])\n",
       "    classes (shape = [1, 2])\n",
       "    schools (shape = [1, 1])\n",
       "    ...And 1 more\n",
       "  Relationships:\n",
       "    transactions.Step Name -> problem_steps.Step Name\n",
       "    problem_steps.Problem Name -> problems.Problem Name\n",
       "    transactions.Class -> classes.Class\n",
       "    classes.School -> schools.School\n",
       "    transactions.Attempt At Step -> attempts.Attempt At Step"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that each branch is a one -> many relationship\n",
    "\n",
    "# schools       students     problems\n",
    "#        \\        |         /\n",
    "#   classes   sessions   problem steps\n",
    "#          \\     |       /\n",
    "#           transactions  -- attempts\n",
    "#\n",
    "\n",
    "es = utilities.learnlab_to_entityset(data)\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Building Features\n",
    "We create a custom primitive: `ProbFail`, which calculates the likelihood that a boolean variable is false. It's worth noting that the opposite of this primitive is built in to Featuretools: `PercentTrue`. One of the many advantages in defining custom primitives is that we can define the name and input types as we would like. If you're interested in creating your own custom primitives for this dataset, copy and modify this step as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from featuretools.primitives import make_agg_primitive\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "def probability(boolean):\n",
    "    numtrue = len([x for x in boolean if x==1])\n",
    "    return 1 - numtrue/len(boolean)\n",
    "\n",
    "ProbFail = make_agg_primitive(probability,\n",
    "                              input_types=[vtypes.Boolean],\n",
    "                              name='failure_rate',\n",
    "                              description='Calculates likelihood a boolean is false over a region',\n",
    "                              return_type=vtypes.Numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate a feature matrix on the `transactions` entity to try to predict the outcome of a given transaction. It's at this step that our previous setup pays off: we can automatically calculate features as if at a given point in time using Deep Feature Synthesis. Furthermore, we can guarentee that future values for `Outcome` won't be used for any calculations because we set the time index of that value to be after the cutoff time.\n",
    "\n",
    "Lastly, we can automatically apply `ProbFail` while grouping by any of the entities we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 402it [00:00, 5109.05it/s]\n",
      "Progress: 100%|██████████| 61/61 [00:50<00:00,  1.20cutoff time/s]\n",
      "Created 177 features\n"
     ]
    }
   ],
   "source": [
    "# Automatically generate features on collected data\n",
    "from featuretools.primitives import Sum, Mean, Median, Count, Hour \n",
    "cutoff_times = es['transactions'].df[['Transaction Id', 'End Time', 'Outcome']]\n",
    "fm, features = ft.dfs(entityset=es, \n",
    "                      target_entity='transactions',\n",
    "                      agg_primitives=[Sum, Mean, ProbFail],\n",
    "                      trans_primitives=[Hour],\n",
    "                      max_depth=3,\n",
    "                      approximate='2m',\n",
    "                      cutoff_time=cutoff_times,\n",
    "                      verbose=True)\n",
    "print('Created {} features'.format(len(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Feature: problem_steps.problems.MEAN(transactions.Feedback Text)>,\n",
       " <Feature: problem_steps.problems.MEAN(transactions.Feedback Classification)>,\n",
       " <Feature: problem_steps.problems.MEAN(transactions.Help Level)>,\n",
       " <Feature: problem_steps.problems.MEAN(transactions.Total Num Hints)>,\n",
       " <Feature: problem_steps.problems.FAILURE_RATE(transactions.Outcome)>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature `problem_steps.problems.FAILURE_RATE(transactions.Outcome)` is exactly the percent of students who did not succeed on a given `problem_step` as calculated at a given time. A feature like `attempts.FAILURE_RATE(transactions.Outcome)` would be the failure rate as grouped by the problem attempt (i.e. more students miss the questions on an earlier attempts than later ones). It's easy to see how a feature like this might be important in predicting the outcome of a given transaction.\n",
    "\n",
    "# Phase 3: Making predictions\n",
    "We were able to add our label `Outcome` to our feature matrix using cutoff times. We now pop it out of the feature matrix and make predictions using a timeseries split and the `roc_auc_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from featuretools.selection import remove_low_information_features\n",
    "fm_enc, _ = ft.encode_features(fm, features)\n",
    "fm_enc = fm_enc.fillna(0)\n",
    "fm_enc = remove_low_information_features(fm_enc)\n",
    "labels = fm.pop('Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6778 features\n",
      "AUC score on time split 0 is 0.54\n",
      "Top 5 features: ['Attempt At Step = 1', 'problem_steps.MEAN(transactions.Duration (sec))', 'Attempt At Step = 2', 'Anon Student Id = unknown', 'attempts.MEAN(transactions.Problem View)']\n",
      "-----\n",
      "\n",
      "AUC score on time split 1 is 0.6\n",
      "Top 5 features: ['Problem View', 'attempts.FAILURE_RATE(transactions.Outcome)', 'problem_steps.MEAN(transactions.Duration (sec))', 'attempts.SUM(transactions.Duration (sec))', 'problem_steps.FAILURE_RATE(transactions.Outcome)']\n",
      "-----\n",
      "\n",
      "AUC score on time split 2 is 0.57\n",
      "Top 5 features: ['Problem View', 'problem_steps.FAILURE_RATE(transactions.Outcome)', 'attempts.SUM(transactions.Problem View)', 'Attempt At Step = 1', 'problem_steps.MEAN(transactions.Duration (sec))']\n",
      "-----\n",
      "\n",
      "AUC score on time split 3 is 0.58\n",
      "Top 5 features: ['Problem View', 'Attempt At Step = 2', 'problem_steps.FAILURE_RATE(transactions.Outcome)', 'Session Id = unknown', 'problem_steps.MEAN(transactions.Duration (sec))']\n",
      "-----\n",
      "\n",
      "AUC score on time split 4 is 0.57\n",
      "Top 5 features: ['Problem View', 'attempts.FAILURE_RATE(transactions.Outcome)', 'problem_steps.FAILURE_RATE(transactions.Outcome)', 'attempts.SUM(transactions.Problem View)', 'Anon Student Id = unknown']\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Using {} features\".format(len(fm_enc)))\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "splitter = TimeSeriesSplit(n_splits=5, max_train_size=None)\n",
    "i=0\n",
    "for train_index, test_index in splitter.split(fm):\n",
    "    clf = RandomForestClassifier()\n",
    "    X_train, X_test = fm_enc.iloc[train_index], fm_enc.iloc[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    score = round(roc_auc_score(preds, y_test), 2)\n",
    "    print(\"AUC score on time split {} is {}\".format(i, score))\n",
    "    feature_imps = [(imp, fm_enc.columns[i]) for i, imp in enumerate(clf.feature_importances_)]\n",
    "    feature_imps.sort()\n",
    "    feature_imps.reverse()\n",
    "    print(\"Top 5 features: {}\".format([f[1] for f in feature_imps[0:5]]))\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Next Steps\n",
    "This notebook showed how to structure your data and make predictions with machine learning. Rather than spending time creating features, it's now possible to explore the relationships and implications betweem thousands of features directly. Reasonable next steps might be to:\n",
    "1. Make plots to better understand the relationship between existing features and the label \n",
    "2. Reduce the total number of features and tune the machine learning model\n",
    "3. Create discipline specific *custom primitives* that might be useful for this prediction problem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
