{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.17'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import featuretools as ft\n",
    "import utilities\n",
    "ft.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Creating a useful dataset structure\n",
    "Since we have so many categorical columns, it's worth taking a moment to think about how this data is structured. At the base level we have `transactions`, every event that is recorded in the data. The columns of those transactions have variables that can be grouped together. As an example, there are only 78 distinct `problem_steps` for the 6778 transactions we have. Associated to each such problem step, we have a variety of knowledge components (KC) and custom fields (CF) associated to that step.\n",
    "\n",
    "We create an entityset structure using the `learnlab_to_entityset` function in [utilities](utilities.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: Dataset\n",
       "  Entities:\n",
       "    transactions (shape = [6778, 26])\n",
       "    problem_steps (shape = [78, 49])\n",
       "    problems (shape = [20, 1])\n",
       "    sessions (shape = [59, 3])\n",
       "    students (shape = [59, 2])\n",
       "    ...And 3 more\n",
       "  Relationships:\n",
       "    transactions.Step Name -> problem_steps.Step Name\n",
       "    problem_steps.Problem Name -> problems.Problem Name\n",
       "    transactions.Session Id -> sessions.Session Id\n",
       "    sessions.Anon Student Id -> students.Anon Student Id\n",
       "    transactions.Class -> classes.Class\n",
       "    ...and 2 more"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/data.txt', '\\t')\n",
    "es = utilities.learnlab_to_entityset(data)\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Building Features\n",
    "We create a custom primitive: `ProbFail`, which calculates the likelihood that a boolean variable is false. It's worth noting that the opposite of this primitive is built in to Featuretools: `PercentTrue`. One of the many advantages in defining custom primitives is that we can define the name and input types as we would like. If you're interested in creating your own custom primitives for this dataset, copy and modify this code as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featuretools.primitives import make_agg_primitive\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "def probability(boolean):\n",
    "    numtrue = len([x for x in boolean if x==1])\n",
    "    return 1 - numtrue/len(boolean)\n",
    "\n",
    "ProbFail = make_agg_primitive(probability,\n",
    "                              input_types=[vtypes.Boolean],\n",
    "                              name='failure_rate',\n",
    "                              description='Calculates likelihood a boolean is false over a region',\n",
    "                              return_type=vtypes.Numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate a feature matrix on the `transactions` entity to try to predict the outcome of a given transaction. It's at this step that our previous setup pays off: we can automatically calculate features as if at a given point in time using Deep Feature Synthesis. Furthermore, we can guarentee that future values for `Outcome` won't be used for any calculations because we set the time index of that value to be after the cutoff time.\n",
    "\n",
    "Lastly, we can automatically apply `Prob` while grouping by any of the entities we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 147it [00:00, 9258.67it/s]\n",
      "Progress: 100%|██████████| 118/118 [01:33<00:00,  1.26cutoff time/s]\n",
      "Created 74 features\n"
     ]
    }
   ],
   "source": [
    "# Automatically generate features on collected data\n",
    "from featuretools.primitives import Sum, Mean, Median, Count, Hour \n",
    "cutoff_times = es['transactions'].df[['Transaction Id', 'End Time', 'Outcome']][500:]\n",
    "fm, features = ft.dfs(entityset=es, \n",
    "                      target_entity='transactions',\n",
    "                      agg_primitives=[ProbFail],\n",
    "                      trans_primitives=[],\n",
    "                      seed_features=[],\n",
    "                      max_depth=3,\n",
    "                      approximate='1m',\n",
    "                      cutoff_time=cutoff_times,\n",
    "                      verbose=True)\n",
    "print('Created {} features'.format(len(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Feature: problem_steps.Problem Name>,\n",
       " <Feature: sessions.Anon Student Id>,\n",
       " <Feature: classes.School>,\n",
       " <Feature: problem_steps.FAILURE_RATE(transactions.Outcome)>,\n",
       " <Feature: sessions.FAILURE_RATE(transactions.Outcome)>,\n",
       " <Feature: attempts.FAILURE_RATE(transactions.Outcome)>,\n",
       " <Feature: problem_steps.problems.FAILURE_RATE(transactions.Outcome)>,\n",
       " <Feature: sessions.students.FAILURE_RATE(transactions.Outcome)>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[-8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse a couple of features. The feature `problem_steps.FAILURE_RATE(transactions.Outcome)` is exactly the percent of students who did not succeed on a given `problem_step` as calculated at a given time. Similarly, the `attempts.FAILURE_RATE(transactions.Outcome)` is the failure rate as grouped by the problem attempt (i.e. more students miss the questions on an earlier attempts than later ones).\n",
    "\n",
    "# Phase 3: Making predictions\n",
    "We were able to add our label `Outcome` to our feature matrix using cutoff times. We now pop it out of the feature matrix and make predictions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featuretools.selection import remove_low_information_features\n",
    "fm_enc, _ = ft.encode_features(fm, features)\n",
    "fm_enc = fm_enc.fillna(0)\n",
    "fm_enc = remove_low_information_features(fm_enc)\n",
    "labels = fm.pop('Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on time split 0 is 0.61\n",
      "Top 5 features: ['sessions.FAILURE_RATE(transactions.Outcome)', 'sessions.students.FAILURE_RATE(transactions.Outcome)', 'attempts.FAILURE_RATE(transactions.Outcome)', 'problem_steps.FAILURE_RATE(transactions.Outcome)', 'problem_steps.problems.FAILURE_RATE(transactions.Outcome)']\n",
      "-----\n",
      "\n",
      "AUC score on time split 1 is 0.6\n",
      "Top 5 features: ['sessions.FAILURE_RATE(transactions.Outcome)', 'sessions.students.FAILURE_RATE(transactions.Outcome)', 'attempts.FAILURE_RATE(transactions.Outcome)', 'problem_steps.FAILURE_RATE(transactions.Outcome)', 'problem_steps.problems.FAILURE_RATE(transactions.Outcome)']\n",
      "-----\n",
      "\n",
      "AUC score on time split 2 is 0.59\n",
      "Top 5 features: ['sessions.students.FAILURE_RATE(transactions.Outcome)', 'sessions.FAILURE_RATE(transactions.Outcome)', 'attempts.FAILURE_RATE(transactions.Outcome)', 'problem_steps.FAILURE_RATE(transactions.Outcome)', 'problem_steps.problems.FAILURE_RATE(transactions.Outcome)']\n",
      "-----\n",
      "\n",
      "AUC score on time split 3 is 0.62\n",
      "Top 5 features: ['sessions.students.FAILURE_RATE(transactions.Outcome)', 'sessions.FAILURE_RATE(transactions.Outcome)', 'attempts.FAILURE_RATE(transactions.Outcome)', 'problem_steps.FAILURE_RATE(transactions.Outcome)', 'problem_steps.problems.FAILURE_RATE(transactions.Outcome)']\n",
      "-----\n",
      "\n",
      "AUC score on time split 4 is 0.61\n",
      "Top 5 features: ['sessions.students.FAILURE_RATE(transactions.Outcome)', 'sessions.FAILURE_RATE(transactions.Outcome)', 'attempts.FAILURE_RATE(transactions.Outcome)', 'problem_steps.FAILURE_RATE(transactions.Outcome)', 'problem_steps.problems.FAILURE_RATE(transactions.Outcome)']\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "splitter = TimeSeriesSplit(n_splits=5, max_train_size=None)\n",
    "i=0\n",
    "for train_index, test_index in splitter.split(fm):\n",
    "    clf = RandomForestClassifier()\n",
    "    X_train, X_test = fm_enc.iloc[train_index], fm_enc.iloc[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    score = round(roc_auc_score(preds, y_test), 2)\n",
    "    print(\"AUC score on time split {} is {}\".format(i, score))\n",
    "    feature_imps = [(imp, fm_enc.columns[i]) for i, imp in enumerate(clf.feature_importances_)]\n",
    "    feature_imps.sort()\n",
    "    feature_imps.reverse()\n",
    "    print(\"Top 5 features: {}\".format([f[1] for f in feature_imps[0:5]]))\n",
    "    print(\"-----\\n\")\n",
    "    \n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
